{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "import pycurl\n",
    "import json\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import http.client as http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting price data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First of all we must get the indicator so we can acquire the info of that indicator from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_recollection(object):\n",
    "    \"\"\"\n",
    "    With this class we are resuming all the steps for getting the data into some functions. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "    def get_list_indicators(self):\n",
    "        \"\"\"\n",
    "        A function to get the list of all the indicators that we can find in the ESIOS API\n",
    "        \"\"\"\n",
    "        import urllib.request\n",
    "        import requests\n",
    "        import json\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        token = '3eae9719f5c8a0dff1c71bb3a6e709bbc37bfce5f6df3662789a1c6fee2ebd67'\n",
    "        #First, we set the url:\n",
    "        url_esios1='https://api.esios.ree.es/indicators'\n",
    "        #After that, we manage to request the dictionary with the indicators from the webpage:\n",
    "        request = urllib.request.Request(url_esios1)\n",
    "        head=[\"Authorization: Token token=\\\"\"+token+\"\\\"\"]\n",
    "        request.add_header(\"Authorization\",\"Token token=\\\"\"+token+\"\\\"\")\n",
    "        response = urllib.request.urlopen(request)\n",
    "        responseStr = str(response.read().decode('utf-8'))\n",
    "\n",
    "        # We fetch json from the response\n",
    "        js = json.loads(responseStr)\n",
    "\n",
    "        dicc=js['indicators']\n",
    "\n",
    "        #We put the results into a list so we can look through it:\n",
    "        busqueda=[]\n",
    "        for diccionario in dicc:\n",
    "            busqueda.append(diccionario)\n",
    "        # Finally, we return de results\n",
    "        return busqueda\n",
    "\n",
    "    def get_indicator(self,indicator,date_today=date.today().strftime(\"%Y-%m-%d\")):\n",
    "        \"\"\"\n",
    "        With this function we will connect to the server of ESIOS and we will get the info of the indicator that we want until the\n",
    "        date that we indicate. As default, it will be set until today. The parameters are:\n",
    "            - indicator: number of the indicator according to the dictionary that we have\n",
    "            - date: limit day for the info. format \"Year-month-day\"\n",
    "        \"\"\"\n",
    "        import http.client as http\n",
    "        import urllib.request\n",
    "        import requests\n",
    "        import json\n",
    "        import datetime as dt\n",
    "        from datetime import date, datetime, timedelta\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        token = '3eae9719f5c8a0dff1c71bb3a6e709bbc37bfce5f6df3662789a1c6fee2ebd67'\n",
    "        # We change the http from 1.1 to 1.0 beacuse it sometimes gives problems when requesting the data \n",
    "        http.HTTPConnection._http_vsn = 10\n",
    "        http.HTTPConnection._http_vsn_str = 'HTTP/1.0'\n",
    "        # Set URL value\n",
    "        url='https://api.esios.ree.es/indicators/'+str(indicator)+'?start_date=2014-04-01T00%3A00%3A00Z&end_date='+date_today+'T23%3A50%3A00Z&groupby=hour'\n",
    "        # Get the request\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"Authorization\",\"Token token=\\\"\"+token+\"\\\"\")\n",
    "        response = urllib.request.urlopen(request)\n",
    "        responseStr = str(response.read().decode('utf-8'))\n",
    "        # Fetch json from the response\n",
    "        data = json.loads(responseStr)\n",
    "        indicators = data['indicator'] \n",
    "        return indicators       \n",
    "        \n",
    "    def get_values(self,data):\n",
    "        \"\"\"\n",
    "        With this function we will manage to get the values of the dictionary and create a dataframe with\n",
    "        the info that we want.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        # First we get the values from the dictionary\n",
    "        data_list = list(data['values'])\n",
    "        # Then we create a df with the values that we are interested in:\n",
    "        value=[]\n",
    "        datetime=[]\n",
    "        datetime_utc=[]\n",
    "        tz_time=[]\n",
    "        geo_id=[]\n",
    "        geo_name=[]\n",
    "        for dic in data_list:\n",
    "            value.append(dic['value'])\n",
    "            datetime.append(dic['datetime'])\n",
    "            datetime_utc.append(dic['datetime_utc'])\n",
    "            tz_time.append(dic['tz_time'])\n",
    "            geo_id.append(dic['geo_id'])\n",
    "            geo_name.append(dic['geo_name'])\n",
    "        #We create the dictionary and change de data types.\n",
    "        df=pd.DataFrame({'value':value,'datetime':datetime,'datetime_utc':datetime_utc,'tz_time':tz_time,'geo_id':geo_id,'geo_name':geo_name},)\n",
    "        df['datetime']=pd.to_datetime(df['datetime'])\n",
    "        df['datetime_utc']=pd.to_datetime(df['datetime_utc'])\n",
    "        df['tz_time']=pd.to_datetime(df['tz_time'])\n",
    "        df=df[(df['geo_name']=='España')|(df['geo_name']=='Península')]\n",
    "        return df    \n",
    "    \n",
    "    def worldbank_info(self,indicator):\n",
    "        \"\"\"\n",
    "        With this function we will get the information necessary from the worldbank api. We just need to add the \n",
    "        indicator and we will get a dataframe with the date, the value and the unit\n",
    "        \"\"\"\n",
    "        import urllib.request\n",
    "        import requests\n",
    "        import json\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "            # Set URL value\n",
    "        url_worldbank=' http://api.worldbank.org/v2/country/all/indicator/'+indicator+'?per_page=20000&format=json'\n",
    "            # Get the request\n",
    "        request = urllib.request.Request(url_worldbank)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        responseStr = str(response.read().decode('utf-8'))\n",
    "            # Fetch json from the response\n",
    "        data = json.loads(responseStr)\n",
    "            # Then we get the values from the json\n",
    "        valor=[]\n",
    "        fecha=[]\n",
    "        unidad=[]\n",
    "        for cell in data[1]:\n",
    "            if cell['country']['value']=='Spain':\n",
    "                valor.append(cell['value'])\n",
    "                fecha.append(cell['date'])\n",
    "            else:\n",
    "                continue\n",
    "            # Create the dataframe with the values.\n",
    "        df = pd.DataFrame({'date':fecha,'value':valor})\n",
    "        df[['value']]=df[['value']].astype(float)\n",
    "        df[['date']]=df[['date']].astype(int)\n",
    "        df2 = df[(df['date']>=2014) & (df['date']<=2020)]\n",
    "        return df2\n",
    "    \n",
    "    def finance_data(self,indicator):\n",
    "        \"\"\"\n",
    "        With this function we will get the stock market historical values from Yahoo! Finance for the indicator we decide.\n",
    "        \"\"\"\n",
    "        import pandas_datareader as pdr\n",
    "        import datetime as dt\n",
    "        from datetime import date, datetime, timedelta\n",
    "        \n",
    "        ree = pdr.data.DataReader(indicator,'yahoo', start=datetime(2014, 4, 1), end=datetime.now())\n",
    "        return ree\n",
    "\n",
    "    def national_holidays(self):\n",
    "        \"\"\"\n",
    "        We will indicate the days that are festive for the whole country:\n",
    "            - 1 de Enero -> Año nuevo\n",
    "            - 6 de Enero -> Reyes - Epifanía del Señor\n",
    "            - 10 de Abril -> Viernes Santo\n",
    "            - 1 de Mayo -> Fiesta del Trabajo\n",
    "            - 15 de Agosto -> Asunción de la Virgen\n",
    "            - 12 de Octubre -> Día de la Hispanidad\n",
    "            - 8 de Diciembre -> Inmaculada Concepción\n",
    "            - 25 de Diciembre -> Navidad\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        festivos=[[1,1,1],[6,1,1],[10,4,1],[1,5,1],[15,8,1],[12,10,1],[8,12,1],[25,12,1]]\n",
    "        df_fest=pd.DataFrame(festivos,columns=['day','month','value'])\n",
    "        return df_fest\n",
    "\n",
    "    def pib_data(self):\n",
    "        \"\"\"\n",
    "        With this function wr are getting the PIB for each trimester\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        pib=pd.DataFrame()\n",
    "        for i in reversed(range(2014,(datetime.today().year+1))):\n",
    "            url='https://datosmacro.expansion.com/pib/espana?anio='+str(i)\n",
    "            df=pd.read_html(url)\n",
    "            pib_anio=df[0]\n",
    "            pib_anio.drop(pib_anio.tail(1).index,inplace=True)\n",
    "            pib=pib.append(pib_anio)\n",
    "        return pib\n",
    "\n",
    "    def last_day_of_month(any_day):\n",
    "        \"\"\"\n",
    "        This is a function that will return the last day of the month you provide\n",
    "        \"\"\"\n",
    "        import datetime as dt\n",
    "        from datetime import date, datetime, timedelta\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        next_month = any_day.replace(day=28) + dt.timedelta(days=4)  # this will never fail\n",
    "        return next_month - dt.timedelta(days=next_month.day)\n",
    "\n",
    "    def get_temperature_aemet(fecha_inicio,fecha_final):\n",
    "        \"\"\"\n",
    "        This is the function used to get the information needed to get the temperature from the AEMET API. \n",
    "        \"\"\"\n",
    "        import urllib.request\n",
    "        import requests\n",
    "        import json\n",
    "        import datetime as dt\n",
    "        from datetime import date, datetime, timedelta\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        def hour_rounder(t):\n",
    "            # Rounds to nearest hour by adding a timedelta hour if minute >= 30\n",
    "            return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour)+timedelta(hours=t.minute//30))\n",
    "        \n",
    "        aemet_api='eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqb2VsLmRlbGFjcnV6ZnVlcnRlc0Bob3RtYWlsLmNvbSIsImp0aSI6IjllYTk2Mzc3LWIxNWItNDAyYS04MmMzLTNjMzVjMzA2ODQ4NCIsImlzcyI6IkFFTUVUIiwiaWF0IjoxNTg4NDMzOTQ5LCJ1c2VySWQiOiI5ZWE5NjM3Ny1iMTViLTQwMmEtODJjMy0zYzM1YzMwNjg0ODQiLCJyb2xlIjoiIn0.rTkcngrv3uJf4RRcJbM14af19pfE5eTT6edG1i-JyFY'\n",
    "        \n",
    "        url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/\"+fecha_inicio+\"T00%3A00%3A00UTC/fechafin/\"+fecha_final+\"T23%3A59%3A00UTC/todasestaciones/\"\n",
    "\n",
    "        querystring = {\"api_key\":aemet_api}\n",
    "\n",
    "        headers = {\n",
    "            'cache-control': \"no-cache\"\n",
    "            }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        js = json.loads(response.text)\n",
    "        #After reading the link we have to go through another link inside:\n",
    "        response2 = requests.request(\"GET\", js['datos'], headers=headers, params=querystring)\n",
    "        js2 = json.loads(response2.text)\n",
    "\n",
    "        # Then we create the dataframe where we will keep the values:\n",
    "        df_temp_aux= pd.DataFrame(columns = ['fecha','datetime','tipo','Temperature'])\n",
    "        # Create a loop for each station:\n",
    "        for estacion in js2:\n",
    "            try:\n",
    "                if (estacion['horatmax']==\"Varias\") | (estacion['horatmin']==\"Varias\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    fecha = estacion['fecha']\n",
    "                    tmax = estacion['tmax']\n",
    "                    tmin = estacion['tmin']\n",
    "                    hora_tmax = estacion['horatmax']\n",
    "                    hora_tmin = estacion['horatmin']\n",
    "\n",
    "                    fecha_tmin = fecha + ' ' + hora_tmin\n",
    "                    fecha_tmax = fecha + ' ' + hora_tmax\n",
    "\n",
    "                    if len(fecha_tmin) == 13:\n",
    "                        fecha_tmin = fecha_tmin + \":00\"\n",
    "                    if len(fecha_tmax) == 13:\n",
    "                        fecha_tmax = fecha_tmax + \":00\"\n",
    "\n",
    "                    fecha_tmin_dt = datetime.strptime(fecha_tmin, \"%Y-%m-%d %H:%M\")\n",
    "                    fec_tmin = hour_rounder(fecha_tmin_dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    fecha_tmax_dt = datetime.strptime(fecha_tmax, \"%Y-%m-%d %H:%M\")\n",
    "                    fec_tmax = hour_rounder(fecha_tmax_dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                    df_temp_aux.loc[len(df_temp_aux)] = [fecha, fec_tmin, \"tmin\", tmin]\n",
    "                    df_temp_aux.loc[len(df_temp_aux)] = [fecha, fec_tmax, \"tmax\", tmax] \n",
    "            except:\n",
    "                continue\n",
    "        # Giving the correcto format for float:\n",
    "        df_temp_aux.Temperature = df_temp_aux.Temperature.str.replace(',','.').astype(float)\n",
    "        # Group the data as said in the document:\n",
    "        df_temp2 = df_temp_aux.groupby(['fecha','datetime','tipo'],as_index=False).mean()\n",
    "        df_temp2['datetime'] = pd.to_datetime(df_temp2['datetime'],errors='coerce')\n",
    "        df_temp2['datetime'] = pd.to_numeric(df_temp2['datetime'],errors='coerce') # GroupBy only works with numeric values\n",
    "        df_temp = df_temp2.groupby(['fecha','tipo'],as_index=False).agg({'datetime':'mean','Temperature':'mean'})\n",
    "        df_temp['datetime'] = pd.to_datetime(df_temp['datetime'], format = \"%Y-%m-%d %H:%M:%S\",errors='coerce', utc=True)\n",
    "        df_temp['datetime'] = df_temp['datetime'].dt.floor('T')\n",
    "        df_temp['datetime'] = df_temp['datetime'].apply(lambda x: hour_rounder(x))\n",
    "        return df_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rec=data_recollection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the class is defined, we are ready to get all the information and manage to manipulate all the tables in order to get our final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Yahoo! Finance info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to show, somehow, the effect of a crisis, we may add the stock market value of IBEX35 to show the evolution of the country. Moreover, we will add the stock market value for the REE as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_indicators=['REE.MC','%5EIBEX']\n",
    "stock_market_dict={'REE.MC':'Red_Electrica',\n",
    "                   '%5EIBEX':'IBEX35'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stock_market_list = [data_rec.finance_data(st) for st in stock_market_indicators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ESIOS info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a list in which every element will be a dataframe, so we will end up with a list of dataframes that we will join later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_list=[10027,600,612,613,614,615,616,617,618,369,370,92,91,79,95,88,90,96,82,81,87,71,72,77,78,74,86,93,94,84,85,89,73]\n",
    "objective=1014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the values for the indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data_rec.get_indicator(ind) for ind in indicators_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataframes_list = [data_rec.get_values(dt) for dt in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_objective=data_rec.get_indicator(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objective=data_rec.get_values(values_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the names of the indicators so we can identify them in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions=data_rec.get_list_indicators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names={}\n",
    "for i in descriptions:\n",
    "    if i['id'] in (indicators_list) :\n",
    "        names[i['id']]=i['name'].replace(' ','_')\\\n",
    "        .replace('á','a')\\\n",
    "        .replace('é','e')\\\n",
    "        .replace('í','i')\\\n",
    "        .replace('ó','o')\\\n",
    "        .replace('ú','u')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# We do the same for the objective name:\n",
    "objective_name={}\n",
    "for i in descriptions:\n",
    "    if i['id']==1014 :\n",
    "        objective_name[i['id']]=i['name'].replace(' ','_')\\\n",
    "        .replace('á','a')\\\n",
    "        .replace('é','e')\\\n",
    "        .replace('í','i')\\\n",
    "        .replace('ó','o')\\\n",
    "        .replace('ú','u')\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the index of each name in our list of dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_indicators=[]\n",
    "for i in names.keys():\n",
    "    if i!=1014:\n",
    "        index_indicators.append(indicators_list.index(i))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_names=list(names.values())\n",
    "for i,ind in enumerate(index_indicators):\n",
    "    dataframes_list[ind].rename(columns={'value':list_of_names[i]},inplace=True)\n",
    "\n",
    "df_objective.rename(columns={'value':objective_name[1014]},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can merge all the dataframes now that we can identify the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(len(dataframes_list)+1)\n",
    "\n",
    "df_esios=df_objective.copy()\n",
    "for i in dataframes_list[:limit]:\n",
    "    df_esios=df_esios.merge(i.iloc[:,0:4],how='left',on=['datetime_utc','datetime','tz_time']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may create some other columns that may be useful for mergin other dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_esios['day']=df_esios['datetime_utc'].dt.day\n",
    "df_esios['month']=df_esios['datetime_utc'].dt.month\n",
    "df_esios['year']=df_esios['datetime_utc'].dt.year\n",
    "df_esios['hour']=df_esios['datetime_utc'].dt.hour\n",
    "df_esios['quarter']=df_esios['datetime_utc'].dt.quarter\n",
    "df_esios['datetime']=pd.to_datetime(df_esios['datetime'],utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. WorldBank info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info of the WorldBank API has been requested related to the industry and electricity sector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldbank_indicators=['FP.CPI.TOTL','IC.ELC.TIME','FP.CPI.TOTL.ZG','SL.IND.EMPL.ZS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldbank_list = [data_rec.worldbank_info(wb) for wb in worldbank_indicators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a dictionary with the name of each indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_worldbank={'FP.CPI.TOTL':'Consumer_price_index',\n",
    "               'IC.ELC.TIME':'Time_required_to_get_electricity_(days)',\n",
    "               'FP.CPI.TOTL.ZG':'Inflation,consumer_prices_(annual_%)',\n",
    "               'SL.IND.EMPL.ZS':'Employment_in_industry_(%_of_total_employment)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get the dataframes, we merge them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(len(worldbank_list)+1)\n",
    "\n",
    "df_esios2=df_esios.copy()\n",
    "for i,datafr in enumerate(worldbank_list[:limit]):\n",
    "    df_esios2=df_esios2.merge(datafr.iloc[:,0:4],how='left',left_on='year',right_on='date').drop_duplicates().drop('date',axis=1)\n",
    "    df_esios2.rename(columns={'value':dict_worldbank[worldbank_indicators[i]]},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. National holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know the national holidays in Spain so we are creating a table with this days to include this info in our final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = data_rec.national_holidays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esios3=df_esios2.merge(holidays,how='left',on=['day','month'])\\\n",
    "                    .rename(columns={'value':'holidays'})\n",
    "\n",
    "df_esios3['holidays'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding the Gross Domestic Product in Spain, so we can include some more economic-social data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = data_rec.pib_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns to merge\n",
    "gdp['quarter'],gdp['str'],gdp['year']=zip(*gdp['Fecha'].str.split())\n",
    "# Replace the values \n",
    "gdp['quarter'].replace({'I':'1','II':'2','III':'3','IV':'4'},inplace=True)\n",
    "# Change the datatype to integer\n",
    "gdp[['quarter','year']]=gdp[['quarter','year']].astype('int')\n",
    "# We drop columns that we don't need\n",
    "gdp.drop('str',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esios4=df_esios3.merge(gdp,how='left',on=['quarter','year'])\\\n",
    "                    .drop(['Fecha','PIB Trimestral.1'],axis=1)\\\n",
    "                    .rename({'PIB Trimestral':'PIB_Trimestral',\n",
    "                            'Var. Trim. PIB (%)':'Var_Trim_PIB_(%)',\n",
    "                            'Var. anual PIB Trim. (%)':'Var_anual_PIB_Trim_(%)'},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Merging Finance info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, the financial data gives problems when requesting them after the esios info, so in order to get everything correct, we are now merging the info that we collected before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(len(stock_market_list)+1)\n",
    "\n",
    "df_esios5=df_esios4.copy()\n",
    "for i,datafr in enumerate(stock_market_list[:limit]):\n",
    "    datafr.index=pd.to_datetime(datafr.index, utc = True)\n",
    "    datafr['Open_hour'] = pd.to_datetime(datafr.index, utc = True) + timedelta(hours = 9)\n",
    "    datafr['Close_hour'] = pd.to_datetime(datafr.index, utc = True) + timedelta(hours = 18)\n",
    "    df_esios5=df_esios5.merge(datafr.iloc[:,[2,6]],how='left',right_on='Open_hour',left_on='datetime').drop_duplicates()\n",
    "    df_esios5=df_esios5.merge(datafr.iloc[:,[3,7]],how='left',right_on='Close_hour',left_on='datetime').drop_duplicates()\n",
    "    df_esios5['Open'] = df_esios5['Open'].fillna(df_esios5['Close'])\n",
    "    df_esios5 = df_esios5.drop(['Close', 'Open_hour', 'Close_hour'],axis=1)\n",
    "    df_esios5.rename(columns={'Open':stock_market_dict[stock_market_indicators[i]]},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the weather we are going to use the AEMET API. it can only be accesed looking for 31 days at once, so we need a loop to get all the information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_aux = pd.DataFrame()\n",
    "for year in range(2014,datetime.today().year+1):\n",
    "    if year==2014:\n",
    "        for month in range(4, 13):\n",
    "            last_day = data_recollection.last_day_of_month(date(year, month, 1)).strftime(\"%Y-%m-%d\")\n",
    "            first_day = date(year, month, 1).strftime(\"%Y-%m-%d\")\n",
    "            df_temp_aux = df_temp_aux.append(data_recollection.get_temperature_aemet(first_day,last_day))\n",
    "    elif (year>2014) & (year<datetime.today().year):\n",
    "        for month in range(1, 13):\n",
    "            last_day = data_recollection.last_day_of_month(date(year, month, 1)).strftime(\"%Y-%m-%d\")\n",
    "            first_day = date(year, month, 1).strftime(\"%Y-%m-%d\")\n",
    "            df_temp_aux = df_temp_aux.append(data_recollection.get_temperature_aemet(first_day,last_day))\n",
    "    else:\n",
    "        for month in range(1, datetime.today().month+1):\n",
    "            last_day = data_recollection.last_day_of_month(date(year, month, 1)).strftime(\"%Y-%m-%d\")\n",
    "            first_day = date(year, month, 1).strftime(\"%Y-%m-%d\")\n",
    "            df_temp_aux = df_temp_aux.append(data_recollection.get_temperature_aemet(first_day,last_day))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp_aux.groupby('datetime',as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_esios5.merge(df_temp[['datetime','Temperature']], how='left', on='datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exporting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('../Exploring_data/TFM_dataframe.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
